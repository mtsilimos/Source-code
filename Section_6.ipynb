{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN3VQUFvbXUM33AjqzkZau7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mtsilimos/Source-code/blob/main/Section_6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# SECTION 6\n",
        "\n",
        "# POS tags\n",
        "\n",
        "import nltk\n",
        "nltk.help.upenn_tagset()\n",
        "nltk.help.upenn_tagset(\"JJ\")\n"
      ],
      "metadata": {
        "id": "5nSONbxzxVxH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# POS tagging\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "from nltk import pos_tag\n",
        "\n",
        "text = \"I make coffee at work.\"\n",
        "tokens = word_tokenize(text)\n",
        "tag = pos_tag(tokens)\n",
        "tag"
      ],
      "metadata": {
        "id": "9OP6e17wxZpN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk import word_tokenize\n",
        "from nltk import pos_tag\n",
        "text = \"a beautifully written novel\"\n",
        "\n",
        "tokens = nltk.word_tokenize(text)\n",
        "tokens\n",
        "\n",
        "tag = nltk.pos_tag(tokens)\n",
        "tag"
      ],
      "metadata": {
        "id": "NVtbXnJRxcue"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# N-GRAMS\n",
        "\n",
        "from nltk import word_tokenize\n",
        "from nltk import pos_tag\n",
        "text = \"I make coffee at work.\"\n",
        "tokens = word_tokenize(text)\n",
        "tag = pos_tag(tokens)\n",
        "tag"
      ],
      "metadata": {
        "id": "6AC-R7wwxfu1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import ngrams\n",
        "bigrams = ngrams(tag, n = 2)\n",
        "\n",
        "for grams in bigrams:\n",
        "  print(grams)"
      ],
      "metadata": {
        "id": "dmGxMT4-x5v8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import ngrams\n",
        "trigrams= ngrams(tag, n = 3)\n",
        "\n",
        "for grams in trigrams:\n",
        "  print(grams)"
      ],
      "metadata": {
        "id": "oCItJ00_x9Xo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Named entity recognition (NER)\n",
        "\n",
        "\n",
        "import nltk\n",
        "from nltk import word_tokenize\n",
        "from nltk import pos_tag\n",
        "text = \"Every day I make coffee for Lisa at Duolingo, located in America, and I pay $3.\"\n",
        "tokens = word_tokenize(text)\n",
        "tag=pos_tag(tokens)\n",
        "tag\n"
      ],
      "metadata": {
        "id": "wGgvvNElyG9j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tree= nltk.ne_chunk(tag)\n",
        "print(tree)\n",
        "tree.draw()"
      ],
      "metadata": {
        "id": "2LDgXB70yK8Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tree = nltk.ne_chunk(tag, binary=True)\n",
        "tree.draw()"
      ],
      "metadata": {
        "id": "EQQpuoG3yOVa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Open and read the text file\n",
        "\n",
        "f = open(r'C:\\Users\\mtsil\\Desktop\\swiss.txt')\n",
        "file = f.read()\n",
        "file"
      ],
      "metadata": {
        "id": "uq4GSXU-yRej"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk import word_tokenize\n",
        "from nltk import sent_tokenize\n",
        "from nltk import pos_tag"
      ],
      "metadata": {
        "id": "EWd4yjKGyVIl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize the text into sentences\n",
        "\n",
        "sentences = sent_tokenize(file)\n",
        "sentences"
      ],
      "metadata": {
        "id": "MVhAOKTMyaFJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize each sentence into words\n",
        "\n",
        "token_sentences = [ word_tokenize(sentence) for sentence in sentences ]\n",
        "\n",
        "print(token_sentences)"
      ],
      "metadata": {
        "id": "RHWE5HSLyehm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Tag each tokenized sentence into parts of speech\n",
        "\n",
        "pos_sentences = [ nltk.pos_tag(sentence) for sentence in token_sentences ]\n",
        "\n",
        "print(pos_sentences)"
      ],
      "metadata": {
        "id": "l0qH9ia3ykPt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract named entities\n",
        "\n",
        "chunked_sentences = nltk.ne_chunk_sents(pos_sentences)\n",
        "\n",
        "for sent in chunked_sentences:\n",
        "  for chunk in sent:\n",
        "    if hasattr(chunk,'label'):\n",
        "      print(chunk)"
      ],
      "metadata": {
        "id": "9GW5Cq9IysC8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualise with a pie chart\n",
        "\n",
        "import collections\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "-__YTshGzYOJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualise with a pie chart\n",
        "\n",
        "chunked_sentences = nltk.ne_chunk_sents(pos_sentences, binary =False)\n",
        "ner_categories=collections.defaultdict(int)\n",
        "\n",
        "for sent in chunked_sentences:\n",
        "  for chunk in sent:\n",
        "    if hasattr(chunk,'label'):\n",
        "      ner_categories[chunk.label()] += 1\n",
        "      print(ner_categories)"
      ],
      "metadata": {
        "id": "c7Y2RDC3zkTc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels = list(ner_categories.keys())\n",
        "values = [ner_categories.get(list) for list in labels]\n",
        "mycolors = [\"lightblue\", \"lightpink\", \"lightyellow\"]\n",
        "plt.pie(values,labels=labels, colors = mycolors, autopct='%1.1f%%', startangle=10)\n",
        "\n",
        "plt.legend(title = \"NER CATEGORIES\", bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0)"
      ],
      "metadata": {
        "id": "VihufFMnztl6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}