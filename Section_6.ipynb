{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPPlOEo0aTE62zfdq6E/jLn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mtsilimos/Source-code/blob/main/Section_6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eGwk1MPsw2t2",
        "outputId": "a85ee09a-9a95-4b81-c7a9-c69b48ceddf7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package tagsets_json to /root/nltk_data...\n",
            "[nltk_data]   Unzipping help/tagsets_json.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "# POS TAGS\n",
        "\n",
        "import nltk\n",
        "nltk.download('tagsets_json')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "from nltk import pos_tag\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "\n",
        "text = \"I make coffee at work.\"\n",
        "tokens = word_tokenize(text)\n",
        "tag = pos_tag(tokens)\n",
        "tag"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YrpV6I0exQgY",
        "outputId": "356dca03-7d68-463e-97b5-f5dacc586344"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('I', 'PRP'),\n",
              " ('make', 'VBP'),\n",
              " ('coffee', 'NN'),\n",
              " ('at', 'IN'),\n",
              " ('work', 'NN'),\n",
              " ('.', '.')]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk import word_tokenize\n",
        "from nltk import pos_tag\n",
        "text = \"a beautifully written novel\"\n",
        "\n",
        "tokens = nltk.word_tokenize(text)\n",
        "tokens\n",
        "\n",
        "tag = nltk.pos_tag(tokens)\n",
        "tag\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bhXJjQK_xcBW",
        "outputId": "2629829a-aa21-4271-b983-c46decd3c624"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('a', 'DT'), ('beautifully', 'RB'), ('written', 'VBN'), ('novel', 'NN')]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# N-GRAMS\n",
        "\n",
        "from nltk import word_tokenize\n",
        "from nltk import pos_tag\n",
        "text = \"I make coffee at work.\"\n",
        "tokens = word_tokenize(text)\n",
        "tag = pos_tag(tokens)\n",
        "tag\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w6pOXDekxew-",
        "outputId": "938db93b-a61f-4393-8292-d5ebdfd52bd1"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('I', 'PRP'),\n",
              " ('make', 'VBP'),\n",
              " ('coffee', 'NN'),\n",
              " ('at', 'IN'),\n",
              " ('work', 'NN'),\n",
              " ('.', '.')]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import ngrams\n",
        "bigrams = ngrams(tag, n = 2)\n",
        "\n",
        "for grams in bigrams:\n",
        "  print(grams)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t91tDa6xxjY3",
        "outputId": "2cf36574-19df-4739-8821-e130be290452"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(('I', 'PRP'), ('make', 'VBP'))\n",
            "(('make', 'VBP'), ('coffee', 'NN'))\n",
            "(('coffee', 'NN'), ('at', 'IN'))\n",
            "(('at', 'IN'), ('work', 'NN'))\n",
            "(('work', 'NN'), ('.', '.'))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import ngrams\n",
        "trigrams= ngrams(tag, n = 3)\n",
        "\n",
        "for grams in trigrams:\n",
        "  print(grams)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6x1GlOEOxltZ",
        "outputId": "4ca13eef-6564-4836-d44b-f92968b420d3"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(('I', 'PRP'), ('make', 'VBP'), ('coffee', 'NN'))\n",
            "(('make', 'VBP'), ('coffee', 'NN'), ('at', 'IN'))\n",
            "(('coffee', 'NN'), ('at', 'IN'), ('work', 'NN'))\n",
            "(('at', 'IN'), ('work', 'NN'), ('.', '.'))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Named entity recognition (NER)\n",
        "\n",
        "\n",
        "import nltk\n",
        "from nltk import word_tokenize\n",
        "from nltk import pos_tag\n",
        "text = \"Every day I make coffee for Lisa at Duolingo, located in America, and I pay $3.\"\n",
        "tokens = word_tokenize(text)\n",
        "tag=pos_tag(tokens)\n",
        "tag\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VwlLxJVhxn6O",
        "outputId": "dbf9c613-2153-4483-81c6-54c65b3e8558"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Every', 'DT'),\n",
              " ('day', 'NN'),\n",
              " ('I', 'PRP'),\n",
              " ('make', 'VBP'),\n",
              " ('coffee', 'NN'),\n",
              " ('for', 'IN'),\n",
              " ('Lisa', 'NNP'),\n",
              " ('at', 'IN'),\n",
              " ('Duolingo', 'NNP'),\n",
              " (',', ','),\n",
              " ('located', 'VBN'),\n",
              " ('in', 'IN'),\n",
              " ('America', 'NNP'),\n",
              " (',', ','),\n",
              " ('and', 'CC'),\n",
              " ('I', 'PRP'),\n",
              " ('pay', 'VBP'),\n",
              " ('$', '$'),\n",
              " ('3', 'CD'),\n",
              " ('.', '.')]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('maxent_ne_chunker_tab')\n",
        "nltk.download('words')\n",
        "tree= nltk.ne_chunk(tag)\n",
        "print(tree)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61955859-01b4-4b8e-f376-66a5c7a73481",
        "id": "9e7aKFw6x9RA"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package maxent_ne_chunker_tab to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker_tab is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(S\n",
            "  Every/DT\n",
            "  day/NN\n",
            "  I/PRP\n",
            "  make/VBP\n",
            "  coffee/NN\n",
            "  for/IN\n",
            "  (PERSON Lisa/NNP)\n",
            "  at/IN\n",
            "  (ORGANIZATION Duolingo/NNP)\n",
            "  ,/,\n",
            "  located/VBN\n",
            "  in/IN\n",
            "  (GPE America/NNP)\n",
            "  ,/,\n",
            "  and/CC\n",
            "  I/PRP\n",
            "  pay/VBP\n",
            "  $/$\n",
            "  3/CD\n",
            "  ./.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tree.pretty_print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q08vgCMW0YeO",
        "outputId": "c660c9d6-75f5-4670-cd82-8ea6c2570526"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                                        S                                                                          \n",
            "    ____________________________________________________________________|____________________________________________________________________       \n",
            "   |       |      |      |         |       |      |    |       |        |    |    |      |      |     |   |    |   PERSON  ORGANIZATION     GPE    \n",
            "   |       |      |      |         |       |      |    |       |        |    |    |      |      |     |   |    |     |          |            |      \n",
            "Every/DT day/NN I/PRP make/VBP coffee/NN for/IN at/IN ,/, located/VBN in/IN ,/, and/CC I/PRP pay/VBP $/$ 3/CD ./. Lisa/NNP Duolingo/NNP America/NNP\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform NER\n",
        "\n",
        "f = open(r'C:\\Users\\mtsil\\Desktop\\swiss.txt')\n",
        "file = f.read()\n",
        "file"
      ],
      "metadata": {
        "id": "VkkUAEImym2d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk import word_tokenize\n",
        "from nltk import sent_tokenize\n",
        "from nltk import pos_tag"
      ],
      "metadata": {
        "id": "xYOoX-H9ytT1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenize the text into sentences\n",
        "\n",
        "sentences = sent_tokenize(file)\n",
        "sentences\n"
      ],
      "metadata": {
        "id": "-CFAB259yxs6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#tokenize each sentence into words\n",
        "\n",
        "token_sentences = [ word_tokenize(sentence) for sentence in sentences ]\n",
        "\n",
        "print(token_sentences)"
      ],
      "metadata": {
        "id": "aPJToq3Ty7md"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tag each tokenized sentence into parts of speech\n",
        "\n",
        "pos_sentences = [ nltk.pos_tag(sentence) for sentence in token_sentences ]\n",
        "\n",
        "print(pos_sentences)"
      ],
      "metadata": {
        "id": "vi6j9bidzA5d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# extract named entities\n",
        "\n",
        "chunked_sentences = nltk.ne_chunk_sents(pos_sentences)\n",
        "\n",
        "for sent in chunked_sentences:\n",
        "  for chunk in sent:\n",
        "    if hasattr(chunk,'label'):\n",
        "      print(chunk)"
      ],
      "metadata": {
        "id": "y4mEq5Y2zF2f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# visualise with a pie chart\n",
        "\n",
        "import collections\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "tj11HWvjzZ8Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#visualise with a pie chart\n",
        "\n",
        "chunked_sentences = nltk.ne_chunk_sents(pos_sentences, binary =False)\n",
        "ner_categories=collections.defaultdict(int)\n",
        "\n",
        "for sent in chunked_sentences:\n",
        "  for chunk in sent:\n",
        "    if hasattr(chunk,'label'):\n",
        "      ner_categories[chunk.label()] += 1\n",
        "print(ner_categories)\n",
        "\n"
      ],
      "metadata": {
        "id": "dNyksQ92zeMl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#visualise with a pie chart\n",
        "\n",
        "labels = list(ner_categories.keys())\n",
        "\n",
        "values = [ner_categories.get(list) for list in labels]\n",
        "mycolors = [\"lightblue\", \"lightpink\", \"lightyellow\"]\n",
        "plt.pie(values,labels=labels, colors = mycolors, autopct='%1.1f%%', startangle=10)\n",
        "plt.legend(title = \"NER CATEGORIES\", bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0)"
      ],
      "metadata": {
        "id": "4nBQ6oJAzvZt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = '''\n",
        "Speaking after talks with German Chancellor Olaf Scholz, Putin said that while the U.S. and NATO rejected Moscowâ€™s demand to keep Ukraine and other ex-Soviet nations out of NATO,\n",
        "halt weapons deployments near Russian borders and roll back alliance forces from Eastern Europe, they have agreed to discuss some security measures already suggested by Russia.\n",
        "'''"
      ],
      "metadata": {
        "id": "v3QQkBd_0Ch-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}