{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# SECTION 1\n",
        "\n",
        "# TOKENIZE A TEXT INTO SENTENCES\n",
        "\n",
        "import nltk\n",
        "\n",
        "from nltk.tokenize import sent_tokenize\n",
        "text = \"I'm making coffee. Would you like one?\"\n",
        "tokens = sent_tokenize(text)\n"
      ],
      "metadata": {
        "id": "TMrAugdzvLLY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens"
      ],
      "metadata": {
        "id": "9vvdUp4yvhHk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(tokens)"
      ],
      "metadata": {
        "id": "kohYG7hzvULc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TOKENIZE A SENTENCE INTO INDIVIDUAL ITEMS\n",
        "\n",
        "import nltk\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "text = \"I'm making coffee.\"\n",
        "tokens = word_tokenize(text)\n"
      ],
      "metadata": {
        "id": "uZG8yBkovaE4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens"
      ],
      "metadata": {
        "id": "0gHiEyX8vivO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(tokens)"
      ],
      "metadata": {
        "id": "murcDa3ovk1o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "$ RegexpTokenizer\n",
        "\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "tokens = RegexpTokenizer('\\w+')\n",
        "print(tokens.tokenize(\"I can't COME NOW.\"))\n"
      ],
      "metadata": {
        "id": "3MtrMqVwvngs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import RegexpTokenizer\n",
        "tokens= RegexpTokenizer('\\w+|\\S')\n",
        "print(tokens.tokenize(\"I can’t COME NOW.\"))\n",
        "\n"
      ],
      "metadata": {
        "id": "uH-E1OzFvwrD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import RegexpTokenizer\n",
        "tokens = RegexpTokenizer('[A-Z]\\w+')\n",
        "print(tokens.tokenize(\"I can't COME NOW.\"))"
      ],
      "metadata": {
        "id": "316f6h4qvxPd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# WordPunktTokenizer\n",
        "\n",
        "from nltk.tokenize import WordPunctTokenizer\n",
        "text=\"p.s. I'd love to come!\"\n",
        "print(WordPunctTokenizer().tokenize(text))\n",
        "\n"
      ],
      "metadata": {
        "id": "8L456ra5v3b_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# WhitespaceTokenizer\n",
        "\n",
        "from nltk.tokenize import WhitespaceTokenizer\n",
        "text = 'Would you like to travel to New York?\\nThe city is expensive\\tbut it is amazing!'\n",
        "print(WhitespaceTokenizer().tokenize(text))\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-vuXmoGSwGxL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TreebankWordTokenizer\n",
        "\n",
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "text= \"If you think you can't keep up-to-date don't @do it! \"\n",
        "print(TreebankWordTokenizer().tokenize(text))\n"
      ],
      "metadata": {
        "id": "ZX5Dlo4JwHSn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize in other languages\n",
        "\n",
        "from nltk.tokenize import sent_tokenize\n",
        "text = 'Θέλω να πάω μια βόλτα. Έχει ζέστη.'\n",
        "greek = sent_tokenize(text, language='greek')\n",
        "greek"
      ],
      "metadata": {
        "id": "E9PRT0epwVPD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 1\n",
        "# Tokenize the following text into sentences and check the length of the tokens.\n",
        "\n",
        "text = 'Dutch social psychologist Geert Hofstede uses the concept of power distance to describe how power is distributed and how hierarchy is perceived in different cultures. In her previous work environment, Gabriela was used to a high power distance  culture where power and authority are respected and everyone has their rightful place. In such a culture, leaders make the big decisions and are not often challenged. Her Swedish team, however, were used to working in a low power distance culture where subordinates often work together with their bosses to find solutions and make decisions. Here, leaders act as coaches or mentors who encourage independent thought and expect to be challenged.'\n",
        "\n"
      ],
      "metadata": {
        "id": "aped7-tnwcRq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 2\n",
        "\n",
        "# Tokenize the following sentence into individual items by using the following methods: a) word.tokenize() b) WordPunkt tokenizer() c) Whitespace tokenizer() and d) TreebankWord tokenizer().\n",
        "# Check and compare the length of tokens for each tokenizer after the application of the four tokenization methods.\n",
        "\n",
        "text = \"If you \\nthink you can't \\tkeep up-to-date don't @do it! \""
      ],
      "metadata": {
        "id": "i6SyDCFawznJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}